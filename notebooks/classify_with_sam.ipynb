{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rainorangelemon/Documents/tiny-diffusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainorangelemon/micromamba/envs/sam2/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainorangelemon/Documents/tiny-diffusion/scripts/best_of_n_on_sam_dataset.py:480: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"../conf\")\n",
      "/home/rainorangelemon/micromamba/envs/sam2/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'clevr_pos': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "from scripts.best_of_n_on_sam_dataset import CLEVRPosDataset\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "with initialize(config_path=\"../conf\", version_base=\"1.1\"):\n",
    "    cfg = compose(config_name=\"clevr_pos\",\n",
    "                  overrides=[\"num_constraints=5\",\n",
    "                             # \"ckpt_path=ComposableDiff/models/ema_runpod_780000.pt\",\n",
    "                             # \"model.noise_schedule=linear\",\n",
    "                            ])\n",
    "\n",
    "dataset = CLEVRPosDataset(data_path=cfg.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rainorangelemon/Documents/sam2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainorangelemon/micromamba/envs/sam2/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n",
    "\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    # label the points with the index\n",
    "    for i, (x, y) in enumerate(coords):\n",
    "        ax.text(x, y, str(i), fontsize=12, color='white', ha='center', va='center')\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "sam2_checkpoint = \"checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classification_with_sam(image, input_points):\n",
    "#     img_batch = [image] * len(input_points)\n",
    "#     predictor.set_image_batch(img_batch)\n",
    "\n",
    "#     input_points = np.array(input_points)\n",
    "#     input_points[:, 0] = input_points[:, 0] * len(image)\n",
    "#     input_points[:, 1] = (1 - input_points[:, 1]) * len(image[0])\n",
    "#     input_labels = np.array([1] * len(input_points))\n",
    "#     draw_labels =  np.array([1] * len(input_points))\n",
    "\n",
    "#     masks_batch, _, _ = predictor.predict_batch(\n",
    "#         point_coords_batch=[p[np.newaxis] for p in input_points],\n",
    "#         point_labels_batch=[l[np.newaxis] for l in input_labels],\n",
    "#         multimask_output=True,\n",
    "#     )\n",
    "\n",
    "#     success = True\n",
    "#     for i, masks in zip(range(len(input_points)), masks_batch):\n",
    "\n",
    "#         success_per_point = ((masks.sum(axis=(1, 2)) > 100) & (masks.sum(axis=(1, 2)) < 1000)).any()\n",
    "#         success = success and success_per_point\n",
    "\n",
    "#         if not success_per_point:\n",
    "#             draw_labels[i] = 0\n",
    "\n",
    "#     return success, masks_batch, draw_labels\n",
    "\n",
    "\n",
    "def classification_with_sam(image, input_points):\n",
    "    anns = mask_generator.generate(image)\n",
    "\n",
    "    input_points = np.array(input_points)\n",
    "    input_points[:, 0] = input_points[:, 0] * len(image)\n",
    "    input_points[:, 1] = (1 - input_points[:, 1]) * len(image[0])\n",
    "    input_labels = np.array([1] * len(input_points))\n",
    "    draw_labels =  np.array([1] * len(input_points))\n",
    "\n",
    "    # get the background mask which has the largest area\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    background_mask = sorted_anns[0]['segmentation']\n",
    "\n",
    "    success = True\n",
    "    for i in range(len(input_points)):\n",
    "\n",
    "        # check if the point is inside the background mask\n",
    "        point = input_points[i]\n",
    "        success_per_point = (background_mask[int(point[1]), int(point[0])] == 0)\n",
    "        success = success and success_per_point\n",
    "\n",
    "        if not success_per_point:\n",
    "            draw_labels[i] = 0\n",
    "\n",
    "    return success, background_mask, draw_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the notebook figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89213836c4ce4a3387181dc66f62edcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b066b5a9fe8f41faa380493eee2e0dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca14a6f8f594d369bbcf5a583d5b28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8bb8b17f354327b2dac109bfb97b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9967a7c800d3422cbb299eb57c37d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245ff917bd9c401eb203259eab23c1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b2534239fe4d949f7ab604fafc3009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108a2a9654364aa791da6ba24f335a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc93604e2694bc39ddb51ce22a63283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fa843f660d4752af1624b245bac04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "successes = []\n",
    "for condition_idx in range(10):\n",
    "    success_per_condition = []\n",
    "    for sample_idx in tqdm(range(100)):\n",
    "        coords, _ = dataset[condition_idx]\n",
    "        # img_path = f\"../tiny-diffusion/runs/10-30_12-00-51_clevr_pos_5_notebook-logp/test_clevr_pos_5000_5/individual_sample_{condition_idx:05d}_{sample_idx}.png\"\n",
    "        img_path = f\"../tiny-diffusion/runs/11-04_10-02-02_clevr_pos_5_notebook-logp/test_clevr_pos_5000_5/individual_sample_{condition_idx:05d}_{sample_idx}.png\"\n",
    "        img = Image.open(img_path)\n",
    "        img = np.array(img)\n",
    "        success, _, draw_labels = classification_with_sam(img, coords)\n",
    "        # plt.close('all')\n",
    "        # plt.clf()\n",
    "        # plt.figure(figsize=(10, 10))\n",
    "        # plt.imshow(img)\n",
    "        # coords = coords.copy()\n",
    "        # coords[:, 0] = coords[:, 0] * len(img)\n",
    "        # coords[:, 1] = (1 - coords[:, 1]) * len(img[0])\n",
    "        # show_points(coords, draw_labels, plt.gca())\n",
    "        # plt.savefig(f\"../tiny-diffusion/runs/10-30_12-00-51_clevr_pos_5_notebook-logp/sam_labels/{condition_idx:05d}_{sample_idx}.png\")\n",
    "\n",
    "        success_per_condition.append(success)\n",
    "    successes.append(success_per_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "np.save(\"../tiny-diffusion/runs/11-04_10-02-02_clevr_pos_5_notebook-logp/sam_labels/successes.npy\", successes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the bash script figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0516b4010949188bfd4f44be3b410c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "successes = []\n",
    "for condition_idx in tqdm(range(5000)):\n",
    "    coords, _ = dataset[condition_idx]\n",
    "    img_path = f\"../tiny-diffusion/runs/rejection_clevr_pos_5/test_clevr_pos_5000_5/sample_{condition_idx:05d}.png\"\n",
    "    img = Image.open(img_path)\n",
    "    img = np.array(img)\n",
    "    success, _, draw_labels = classification_with_sam(img, coords)\n",
    "    successes.append(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9545004945598418)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_energies = torch.load(\"../tiny-diffusion/runs/rejection_clevr_pos_5/test_clevr_pos_5000_5/packed_energies.pt\")\n",
    "np.array(successes)[~(packed_energies > 0).any(dim=1).all(dim=1)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9026)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(successes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../tiny-diffusion/runs/11-05_14-52-32_clevr_pos_5_notebook-logp/test_clevr_pos_5000_5/sam_labels.npy\", successes)\n",
    "np.save(\"../tiny-diffusion/runs/rejection_clevr_pos_5/test_clevr_pos_5000_5/sam_labels.npy\", successes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
